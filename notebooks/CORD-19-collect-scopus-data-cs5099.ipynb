{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORD-19 Collect SCOPUS data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, this jupyter notebook is designated to collect additional data via scopus to enbroaden the CORD19 dataset: \n",
    "https://datadryad.org/stash/dataset/doi:10.5061/dryad.vmcvdncs0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, relevant packages must be imported to the Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import ast\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import Levenshtein as lev\n",
    "from fuzzywuzzy import fuzz \n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter\n",
    "\n",
    "from elsapy.elsclient import ElsClient\n",
    "from elsapy.elsdoc import FullDoc, AbsDoc\n",
    "from elsapy.elssearch import ElsSearch\n",
    "\n",
    "import time # for sleep\n",
    "from pybtex.database import parse_file, BibliographyData, Entry\n",
    "import json\n",
    "from elsapy.elsclient import ElsClient\n",
    "from elsapy.elsdoc import AbsDoc\n",
    "from elsapy.elssearch import ElsSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data and save it to a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORD19_CSV = pd.read_csv('../data/cord-19/CORD19_software_mentions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the length of the column containing doi's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77448"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CORD19_CSV['doi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the column doi to see if there are inconsistencies such as NaN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                 NaN\n",
       "1          10.1016/j.regg.2021.01.002\n",
       "2           10.1016/j.rec.2020.08.002\n",
       "3        10.1016/j.vetmic.2006.11.026\n",
       "4                   10.3390/v12080849\n",
       "                     ...             \n",
       "77443      10.1007/s11229-020-02869-9\n",
       "77444                             NaN\n",
       "77445     10.1101/2020.05.13.20100206\n",
       "77446      10.1007/s42991-020-00052-8\n",
       "77447     10.1101/2020.09.14.20194670\n",
       "Name: doi, Length: 77448, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doi = CORD19_CSV['doi']\n",
    "doi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a series with solely unique values and neglect NaN's. It is important to sort the unique values. Otherwise, the method is creating different results after each restart of the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.1001/jamainternmed.2020.1369       1\n",
       "10.1001/jamanetworkopen.2020.16382    1\n",
       "10.1001/jamanetworkopen.2020.17521    1\n",
       "10.1001/jamanetworkopen.2020.20485    1\n",
       "10.1001/jamanetworkopen.2020.24984    1\n",
       "                                     ..\n",
       "10.9745/ghsp-d-20-00115               1\n",
       "10.9745/ghsp-d-20-00171               1\n",
       "10.9745/ghsp-d-20-00218               1\n",
       "10.9758/cpn.2020.18.4.607             1\n",
       "10.9781/ijimai.2020.02.002            1\n",
       "Name: doi, Length: 74302, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doi_counted = doi.value_counts().sort_index(ascending=True)\n",
    "doi_counted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function determines the requested information from the Scopus API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adapted from https://github.com/ElsevierDev/elsapy/blob/master/exampleProg.py\n",
    "def fetch_scopus_api(client, doi):\n",
    "    \"\"\"obtain additional paper information from scopus by doi\n",
    "    \"\"\"\n",
    "    doc_srch = ElsSearch(\"DOI(\"+doi+\")\",'scopus')\n",
    "    doc_srch.execute(client, get_all = True)\n",
    "    try:\n",
    "        scopus_id=doc_srch.results[0][\"dc:identifier\"].split(\":\")[1]\n",
    "        scp_doc = AbsDoc(scp_id = scopus_id)\n",
    "        if scp_doc.read(client):\n",
    "            scp_doc.write()   \n",
    "        else:\n",
    "            print (\"Read document failed.\")\n",
    "        return scp_doc.data\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thusly, the configuration file is set up and contains an APIkey. Further information: https://github.com/ElsevierDev/elsapy/blob/master/CONFIG.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_file = open(\"config.json\")\n",
    "config = json.load(con_file)\n",
    "con_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, the client is initialized with the API-Key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ElsClient(config['apikey'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstation purposes, the following cells shows which data is returned by the Scopus API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"affiliation\": [\n",
      "    {\n",
      "      \"affiliation-city\": \"New Delhi\",\n",
      "      \"affilname\": \"Jamia Hamdard\",\n",
      "      \"affiliation-country\": \"India\"\n",
      "    },\n",
      "    {\n",
      "      \"affiliation-city\": \"New Delhi\",\n",
      "      \"affilname\": \"Jamia Millia Islamia\",\n",
      "      \"affiliation-country\": \"India\"\n",
      "    },\n",
      "    {\n",
      "      \"affiliation-city\": \"New Delhi\",\n",
      "      \"affilname\": \"Indraprastha Apollo Hospitals\",\n",
      "      \"affiliation-country\": \"India\"\n",
      "    }\n",
      "  ],\n",
      "  \"coredata\": {\n",
      "    \"srctype\": \"j\",\n",
      "    \"eid\": \"2-s2.0-85083171050\",\n",
      "    \"pubmed-id\": \"32305024\",\n",
      "    \"prism:coverDate\": \"2020-07-01\",\n",
      "    \"prism:aggregationType\": \"Journal\",\n",
      "    \"prism:url\": \"https://api.elsevier.com/content/abstract/scopus_id/85083171050\",\n",
      "    \"dc:creator\": {\n",
      "      \"author\": [\n",
      "        {\n",
      "          \"ce:given-name\": \"Raju\",\n",
      "          \"preferred-name\": {\n",
      "            \"ce:given-name\": \"Raju\",\n",
      "            \"ce:initials\": \"R.\",\n",
      "            \"ce:surname\": \"Vaishya\",\n",
      "            \"ce:indexed-name\": \"Vaishya R.\"\n",
      "          },\n",
      "          \"@seq\": \"1\",\n",
      "          \"ce:initials\": \"R.\",\n",
      "          \"@_fa\": \"true\",\n",
      "          \"affiliation\": {\n",
      "            \"@id\": \"60019286\",\n",
      "            \"@href\": \"https://api.elsevier.com/content/affiliation/affiliation_id/60019286\"\n",
      "          },\n",
      "          \"ce:surname\": \"Vaishya\",\n",
      "          \"@auid\": \"6602902951\",\n",
      "          \"author-url\": \"https://api.elsevier.com/content/author/author_id/6602902951\",\n",
      "          \"ce:indexed-name\": \"Vaishya R.\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"link\": [\n",
      "      {\n",
      "        \"@_fa\": \"true\",\n",
      "        \"@rel\": \"self\",\n",
      "        \"@href\": \"https://api.elsevier.com/content/abstract/scopus_id/85083171050\"\n",
      "      },\n",
      "      {\n",
      "        \"@_fa\": \"true\",\n",
      "        \"@rel\": \"scopus\",\n",
      "        \"@href\": \"https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85083171050&origin=inward\"\n",
      "      },\n",
      "      {\n",
      "        \"@_fa\": \"true\",\n",
      "        \"@rel\": \"scopus-citedby\",\n",
      "        \"@href\": \"https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85083171050&origin=inward\"\n",
      "      }\n",
      "    ],\n",
      "    \"source-id\": \"5700165201\",\n",
      "    \"pii\": \"S1871402120300771\",\n",
      "    \"citedby-count\": \"205\",\n",
      "    \"prism:volume\": \"14\",\n",
      "    \"subtype\": \"ar\",\n",
      "    \"dc:title\": \"Artificial Intelligence (AI) applications for COVID-19 pandemic\",\n",
      "    \"openaccess\": \"1\",\n",
      "    \"prism:issn\": \"18780334 18714021\",\n",
      "    \"prism:issueIdentifier\": \"4\",\n",
      "    \"subtypeDescription\": \"Article\",\n",
      "    \"prism:publicationName\": \"Diabetes and Metabolic Syndrome: Clinical Research and Reviews\",\n",
      "    \"prism:pageRange\": \"337-339\",\n",
      "    \"prism:endingPage\": \"339\",\n",
      "    \"openaccessFlag\": \"true\",\n",
      "    \"prism:doi\": \"10.1016/j.dsx.2020.04.012\",\n",
      "    \"prism:startingPage\": \"337\",\n",
      "    \"dc:identifier\": \"SCOPUS_ID:85083171050\",\n",
      "    \"dc:publisher\": \"Elsevier Ltd\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "return_example = fetch_scopus_api(client, '10.1016/j.dsx.2020.04.012')\n",
    "print(json.dumps(return_example, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the returned data, further analysis is conductable. Therefore, two notebooks are created to analyse data linked to: \n",
    "<ul>\n",
    "  <li>affiliation</li>\n",
    "  <li>coredata</li>\n",
    "</ul>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transform the dataset to an ethically correct form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_unethical_entries(json_holder):\n",
    "    #Checking if JSON is None\n",
    "    if json_holder is None:\n",
    "        return json_holder\n",
    "    \n",
    "     #Checking if JSON is NaN\n",
    "    if json_holder == \"NaN\":\n",
    "        return None\n",
    "\n",
    "    #JSON starts with { else [\n",
    "    string_helper = str(json_holder)\n",
    "    #print(string_helper[0])\n",
    "    if string_helper[0] == \"{\":\n",
    "            if 'affiliation-city' in json_holder:\n",
    "                del json_holder['affiliation-city']\n",
    "            if 'affilname' in json_holder:\n",
    "                 del json_holder['affilname']\n",
    "            if 'dc:creator' in json_holder:\n",
    "                 del json_holder['dc:creator']\n",
    "            return json_holder\n",
    "    elif string_helper[0] == \"[\":\n",
    "        #print(json_holder)\n",
    "        for element in json_holder: \n",
    "            if 'affiliation-city' in element.keys():\n",
    "                del element['affiliation-city']\n",
    "            if 'affilname' in element.keys():\n",
    "                 del element['affilname']\n",
    "            if 'dc:creator' in element.keys():\n",
    "                 del element['dc:creator']\n",
    "        return json_holder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform_to_ethical_correct_data(df):\n",
    "    df_index = df.index\n",
    "    len_df_index = len(df_index)\n",
    "    i = 0\n",
    "\n",
    "    while i < len_df_index:\n",
    "        print(\"Progress: \" + str(i+1) +\"/\"+str(len_df_index) + \" Index position: \" + str(df_index[i]) +  \" (Data cleaning according to to ethical guidelines)\")\n",
    "        df['affiliation'][df_index[i]] = remove_unethical_entries(df['affiliation'][df_index[i]])\n",
    "        df['coredata'][df_index[i]] = remove_unethical_entries(df['coredata'][df_index[i]])\n",
    "        i = i + 1\n",
    "    return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thusly, the already fetched coredata and affiliation are read and combined to a DataFrame for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_current_extra_info = pd.DataFrame()\n",
    "bool_show_df = False\n",
    "try:\n",
    "    read_affiliation = pd.read_pickle('extra_info_affiliation_CS.pkl')\n",
    "    read_coredata = pd.read_pickle('extra_info_coredata_CS.pkl')\n",
    "    df_current_extra_info['affiliation'] = read_affiliation\n",
    "    df_current_extra_info['coredata'] = read_coredata\n",
    "    bool_show_df = True\n",
    "except:\n",
    "    print(\"The DataFrame is empty\")\n",
    "    bool_show_df = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Show already fetched information if it is existent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             affiliation  \\\n",
      "0      [{'affiliation-country': 'United States'}, {'a...   \n",
      "1      [{'affiliation-country': 'United States'}, {'a...   \n",
      "2      [{'affiliation-country': 'United States'}, {'a...   \n",
      "3      [{'affiliation-country': 'United States'}, {'a...   \n",
      "4      [{'affiliation-country': 'United States'}, {'a...   \n",
      "...                                                  ...   \n",
      "74297           {'affiliation-country': 'United States'}   \n",
      "74298  [{'affiliation-country': 'United States'}, {'a...   \n",
      "74299           {'affiliation-country': 'United States'}   \n",
      "74300  [{'affiliation-country': 'Turkey'}, {'affiliat...   \n",
      "74301                                               None   \n",
      "\n",
      "                                                coredata  \n",
      "0      {'srctype': 'j', 'eid': '2-s2.0-85083266658', ...  \n",
      "1      {'srctype': 'j', 'prism:issueIdentifier': '7',...  \n",
      "2      {'srctype': 'j', 'prism:issueIdentifier': '8',...  \n",
      "3      {'srctype': 'j', 'prism:issueIdentifier': '9',...  \n",
      "4      {'srctype': 'j', 'prism:issueIdentifier': '11'...  \n",
      "...                                                  ...  \n",
      "74297  {'srctype': 'j', 'eid': '2-s2.0-85092678139', ...  \n",
      "74298  {'srctype': 'j', 'eid': '2-s2.0-85087468210', ...  \n",
      "74299  {'srctype': 'j', 'eid': '2-s2.0-85092677974', ...  \n",
      "74300  {'srctype': 'j', 'prism:issueIdentifier': '4',...  \n",
      "74301                                               None  \n",
      "\n",
      "[74302 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "if bool_show_df == True: \n",
    "    print(df_current_extra_info)\n",
    "else:\n",
    "    print(\"There is no fetched information available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "affiliation                                                 None\n",
       "coredata       {'srctype': 'j', 'prism:issueIdentifier': '1',...\n",
       "Name: 43, dtype: object"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_current_extra_info.loc[43]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of the DataFrame containing the current information is assigned to a variable to be used for further processing. \n",
    "Therefore, the length will be used within a while loop as a starting index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74302"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_df_current_extra_info = len(df_current_extra_info)\n",
    "len_df_current_extra_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def contains_only_None(dic):\n",
    "    \"\"\"\n",
    "    This functions inspects an dictionary and returns True if it solely contains None values\n",
    "    \"\"\"\n",
    "    return len(dic) == sum(value == None for value in dic.values())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transpose_and_structure(df):\n",
    "    df = df.T\n",
    "    if 'affiliation' not in df.columns:\n",
    "        df['affiliation'] = None\n",
    "    if 'coredata' not in df.columns:\n",
    "        df['coredata'] = None\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_fetched_data_to_df(df_current_extra_info, dic):\n",
    "    \"\"\"\n",
    "    This function appends or inserts newly fetched data to the DataFrame containing scopus data.\n",
    "    \"\"\"\n",
    "    #df_current_extra_info -> holding the latest data, new data needs to be appended to it, \n",
    "    #df_newly_fetched_transposed -> holdy newly fetched data, needs to be inserted or fetched\n",
    "    \n",
    "    if contains_only_None(dic):\n",
    "        placeholder_entries = pd.DataFrame(np.empty((len(dict_new_extra_info),2),dtype=object),columns=['affiliation','coredata'], index=dict_new_extra_info.keys())\n",
    "        df_newly_fetched_transposed = placeholder_entries\n",
    "        print(df_newly_fetched_transposed)\n",
    "    else:\n",
    "        #Prior appending, the dictionary is transformed to a DataFrame\n",
    "        df_newly_fetched = pd.DataFrame(dic)\n",
    "        #For readability, the DataFrame is transposed\n",
    "        df_newly_fetched_transposed = transpose_and_structure(df_newly_fetched)\n",
    "        df_newly_fetched_transposed = transform_to_ethical_correct_data(df_newly_fetched_transposed)\n",
    "        print(df_newly_fetched_transposed)\n",
    "    \n",
    "    #Insert newly fetched rows which were previously not successful appended\n",
    "    for index, row in df_newly_fetched_transposed.iterrows():\n",
    "        #insert to current extra info DataFrame because the row is existent\n",
    "        if index in df_current_extra_info.index and row is not None:\n",
    "            df_current_extra_info.loc[index] = row\n",
    "        #append to current extra info DataFrame because the row is new     \n",
    "        if index not in df_current_extra_info.index:\n",
    "            df_current_extra_info = df_current_extra_info.append(row, ignore_index=True)\n",
    "            \n",
    "    #returning DataFrame with inserted and replaced rows. \n",
    "    return df_current_extra_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both Dataframes columns are stored each to an object. The series objects are stored to each to a pkl-file which is not exceeding the size of 100MB allowing GitHub uploads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_df_columns(df):\n",
    "    #df = transform_to_ethical_correct_data(df)\n",
    "    ser_affiliation = df['affiliation']\n",
    "    ser_coredata = df['coredata']\n",
    "    ser_affiliation.to_pickle('extra_info_affiliation_CS.pkl')\n",
    "    ser_coredata.to_pickle('extra_info_coredata_CS.pkl')\n",
    "    df.to_pickle('extra_info_CS5099.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, the fetched scopus data is stored within a dictionary. Besides, the print function is used to show the state of the process by displaying the latest fetched information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "dict_new_extra_info = dict()\n",
    "len_dois = len(doi_counted)\n",
    "def trigger_fetching():\n",
    "    threshold = 0 \n",
    "    i = len_df_current_extra_info\n",
    "    while i < len_dois: # i < len_dois\n",
    "        dict_new_extra_info[i] = fetch_scopus_api(client, doi_counted.index[i])\n",
    "        print(\"Fetching index position: \" + str(i) + \" -> \" +  doi_counted.index[i])\n",
    "        i = i + 1 \n",
    "        threshold = threshold + 1\n",
    "        if threshold > 99:\n",
    "            df_combined_extra_info = append_fetched_data_to_df(df_current_extra_info, dict_new_extra_info)\n",
    "            store_df_columns(df_combined_extra_info)\n",
    "            threshold = 0\n",
    "            print(\"batch saved\")\n",
    "trigger_fetching()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is useful when the process above is interrupted. Therefore, the dictionary containing fetched information can be narrowed down to useful entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def save_new_extra_info(len_df_current_extra_info, upto):\n",
    "#     \"\"\"\n",
    "#     This function is used to separate successfull API calls from API calls which were prevented due to an invalid API-Key.\n",
    "#     As a result, this function returns a range of valid entries up to the given parameter. \n",
    "#     \"\"\"\n",
    "#     dict_new_extra_info_saver = dict()\n",
    "#     i = len_df_current_extra_info\n",
    "#     while i < upto:\n",
    "#         #print(\"Position: \" + str(i) + \" -> \" +  doi_counted.index[i])\n",
    "#         dict_new_extra_info_saver[i] = dict_new_extra_info[i]\n",
    "#         i = i + 1 \n",
    "#     return dict_new_extra_info_saver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The existing and newly fetched information are combined into one DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [affiliation, coredata]\n",
      "Index: []\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>affiliation</th>\n",
       "      <th>coredata</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[{'affiliation-country': 'United States'}, {'a...</td>\n",
       "      <td>{'srctype': 'j', 'eid': '2-s2.0-85083266658', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[{'affiliation-country': 'United States'}, {'a...</td>\n",
       "      <td>{'srctype': 'j', 'prism:issueIdentifier': '7',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[{'affiliation-country': 'United States'}, {'a...</td>\n",
       "      <td>{'srctype': 'j', 'prism:issueIdentifier': '8',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[{'affiliation-country': 'United States'}, {'a...</td>\n",
       "      <td>{'srctype': 'j', 'prism:issueIdentifier': '9',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[{'affiliation-country': 'United States'}, {'a...</td>\n",
       "      <td>{'srctype': 'j', 'prism:issueIdentifier': '11'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74297</th>\n",
       "      <td>{'affiliation-country': 'United States'}</td>\n",
       "      <td>{'srctype': 'j', 'eid': '2-s2.0-85092678139', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74298</th>\n",
       "      <td>[{'affiliation-country': 'United States'}, {'a...</td>\n",
       "      <td>{'srctype': 'j', 'eid': '2-s2.0-85087468210', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74299</th>\n",
       "      <td>{'affiliation-country': 'United States'}</td>\n",
       "      <td>{'srctype': 'j', 'eid': '2-s2.0-85092677974', ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74300</th>\n",
       "      <td>[{'affiliation-country': 'Turkey'}, {'affiliat...</td>\n",
       "      <td>{'srctype': 'j', 'prism:issueIdentifier': '4',...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74301</th>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>74302 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             affiliation  \\\n",
       "0      [{'affiliation-country': 'United States'}, {'a...   \n",
       "1      [{'affiliation-country': 'United States'}, {'a...   \n",
       "2      [{'affiliation-country': 'United States'}, {'a...   \n",
       "3      [{'affiliation-country': 'United States'}, {'a...   \n",
       "4      [{'affiliation-country': 'United States'}, {'a...   \n",
       "...                                                  ...   \n",
       "74297           {'affiliation-country': 'United States'}   \n",
       "74298  [{'affiliation-country': 'United States'}, {'a...   \n",
       "74299           {'affiliation-country': 'United States'}   \n",
       "74300  [{'affiliation-country': 'Turkey'}, {'affiliat...   \n",
       "74301                                               None   \n",
       "\n",
       "                                                coredata  \n",
       "0      {'srctype': 'j', 'eid': '2-s2.0-85083266658', ...  \n",
       "1      {'srctype': 'j', 'prism:issueIdentifier': '7',...  \n",
       "2      {'srctype': 'j', 'prism:issueIdentifier': '8',...  \n",
       "3      {'srctype': 'j', 'prism:issueIdentifier': '9',...  \n",
       "4      {'srctype': 'j', 'prism:issueIdentifier': '11'...  \n",
       "...                                                  ...  \n",
       "74297  {'srctype': 'j', 'eid': '2-s2.0-85092678139', ...  \n",
       "74298  {'srctype': 'j', 'eid': '2-s2.0-85087468210', ...  \n",
       "74299  {'srctype': 'j', 'eid': '2-s2.0-85092677974', ...  \n",
       "74300  {'srctype': 'j', 'prism:issueIdentifier': '4',...  \n",
       "74301                                               None  \n",
       "\n",
       "[74302 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_combined_extra_info = append_fetched_data_to_df(df_current_extra_info, dict_new_extra_info)\n",
    "df_combined_extra_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both Dataframes columns are stored each to an object. The series objects are stored to each to a pkl-file which is not exceeding the size of 100MB allowing GitHub uploads."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying that the returned None values are due to non existent data and not to an invalid API-Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_data():\n",
    "    \"\"\"\n",
    "    This function fetches again the scopus API and solely asks for information which previously returned None. \n",
    "    \"\"\"\n",
    "    #Add a new column to the DataFrame containg the DOI's which are used to fetch the API\n",
    "    ser_doi = pd.Series(doi_counted.index[:len_data])\n",
    "    df_current_extra_info_checker = df_combined_extra_info\n",
    "    df_current_extra_info_checker['doi'] = ser_doi\n",
    "    \n",
    "    #fetching solely none entries \n",
    "    len_df_current_extra_info_checker = len(df_current_extra_info_checker)\n",
    "    dict_new_extra_info_checker = dict()\n",
    "    len_df_current_extra_info_checker = 50\n",
    "    i = 0\n",
    "    while i < len_df_current_extra_info_checker: \n",
    "        if df_current_extra_info_checker['affiliation'][i] == None or df_current_extra_info_checker['coredata'][i] == None:\n",
    "            dict_new_extra_info_checker[i] = fetch_scopus_api(client, ser_doi[i])\n",
    "            print(\"Fetched again index position: \" + str(i) + \" -> \" +  ser_doi[i])\n",
    "        i = i + 1\n",
    "\n",
    "    #check if at least one of the fetched values is not None, otherwise the process is finished\n",
    "    if contains_only_None(dict_new_extra_info_checker):\n",
    "        print(\"The scopus API did not returned new information for existing None values.\")\n",
    "    else:\n",
    "        #There is new information to insert to the existing DataFrame\n",
    "        df_combined_extra_info_fetched_again  = append_fetched_data_to_df(df_current_extra_info, dict_new_extra_info_checker)\n",
    "        store_df_columns(df_combined_extra_info_fetched_again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74302"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_dois = len(doi_counted)\n",
    "len_dois"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74302"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_data = len(df_combined_extra_info)\n",
    "len_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetched again index position: 13 -> 10.1002/0471142700.nc0430s27\n",
      "Fetched again index position: 15 -> 10.1002/0471142735.im0700s95\n",
      "Fetched again index position: 16 -> 10.1002/0471142735.im0700s97\n",
      "Fetched again index position: 43 -> 10.1002/2211-5463.13058\n",
      "Progress: 1/4 Index position: 13 (Data cleaning according to to ethical guidelines)\n",
      "Progress: 2/4 Index position: 15 (Data cleaning according to to ethical guidelines)\n",
      "Progress: 3/4 Index position: 16 (Data cleaning according to to ethical guidelines)\n",
      "Progress: 4/4 Index position: 43 (Data cleaning according to to ethical guidelines)\n",
      "                                             coredata affiliation\n",
      "13                                               None        None\n",
      "15                                               None        None\n",
      "16                                               None        None\n",
      "43  {'srctype': 'j', 'prism:issueIdentifier': '1',...        None\n"
     ]
    }
   ],
   "source": [
    "if len_dois == len_data:\n",
    "    enrich_data()\n",
    "else:\n",
    "    print(\"There are entries which are not fetched yet from the scopus API.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
