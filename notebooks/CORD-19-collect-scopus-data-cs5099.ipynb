{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CORD-19-collect-scopus-data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, this jupyter notebook is designated to collect additional data via scopus to enbroaden the CORD19 dataset: \n",
    "https://datadryad.org/stash/dataset/doi:10.5061/dryad.vmcvdncs0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, relevant packages must be imported to the Notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import csv\n",
    "import ast\n",
    "import collections\n",
    "import matplotlib.pyplot as plt\n",
    "import Levenshtein as lev\n",
    "from fuzzywuzzy import fuzz \n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from urllib.parse import urlparse\n",
    "from collections import Counter\n",
    "\n",
    "from elsapy.elsclient import ElsClient\n",
    "from elsapy.elsdoc import FullDoc, AbsDoc\n",
    "from elsapy.elssearch import ElsSearch\n",
    "\n",
    "import time # for sleep\n",
    "from pybtex.database import parse_file, BibliographyData, Entry\n",
    "import json\n",
    "from elsapy.elsclient import ElsClient\n",
    "from elsapy.elsdoc import AbsDoc\n",
    "from elsapy.elssearch import ElsSearch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the data and save it to a variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORD19_CSV = pd.read_csv('../data/cord-19/CORD19_software_mentions.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check the length of the column containing doi's."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "77448"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(CORD19_CSV['doi'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the column doi to see if there are inconsistencies such as NaN's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                 NaN\n",
       "1          10.1016/j.regg.2021.01.002\n",
       "2           10.1016/j.rec.2020.08.002\n",
       "3        10.1016/j.vetmic.2006.11.026\n",
       "4                   10.3390/v12080849\n",
       "                     ...             \n",
       "77443      10.1007/s11229-020-02869-9\n",
       "77444                             NaN\n",
       "77445     10.1101/2020.05.13.20100206\n",
       "77446      10.1007/s42991-020-00052-8\n",
       "77447     10.1101/2020.09.14.20194670\n",
       "Name: doi, Length: 77448, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doi = CORD19_CSV['doi']\n",
    "doi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a series with solely unique values and neglect NaN's. It is important to sort the unique values. Otherwise, the method is creating different results after each restart of the notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.1001/jamainternmed.2020.1369       1\n",
       "10.1001/jamanetworkopen.2020.16382    1\n",
       "10.1001/jamanetworkopen.2020.17521    1\n",
       "10.1001/jamanetworkopen.2020.20485    1\n",
       "10.1001/jamanetworkopen.2020.24984    1\n",
       "                                     ..\n",
       "10.9745/ghsp-d-20-00115               1\n",
       "10.9745/ghsp-d-20-00171               1\n",
       "10.9745/ghsp-d-20-00218               1\n",
       "10.9758/cpn.2020.18.4.607             1\n",
       "10.9781/ijimai.2020.02.002            1\n",
       "Name: doi, Length: 74302, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doi_counted = doi.value_counts().sort_index(ascending=True)\n",
    "doi_counted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function determines the requested information from the Scopus API. (https://api.elsevier.com/content/search/scopus?query=DOI(10.1109/MCOM.2016.7509373)&apiKey=6d485ef1fe1408712f37e8a783a285a4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adapted from https://github.com/ElsevierDev/elsapy/blob/master/exampleProg.py\n",
    "def AffiliationsFromScopusByDOI(client, doi):\n",
    "    \"\"\"obtain additional paper information from scopus by doi\n",
    "    \"\"\"\n",
    "    doc_srch = ElsSearch(\"DOI(\"+doi+\")\",'scopus')\n",
    "    doc_srch.execute(client, get_all = True)\n",
    "    #print (\"doc_srch has\", len(doc_srch.results), \"results.\")\n",
    "    #print(doc_srch.results)\n",
    "    try:\n",
    "        scopus_id=doc_srch.results[0][\"dc:identifier\"].split(\":\")[1]\n",
    "        scp_doc = AbsDoc(scp_id = scopus_id)\n",
    "        if scp_doc.read(client):\n",
    "            # print (\"scp_doc.title: \", scp_doc.title)\n",
    "            scp_doc.write()   \n",
    "        else:\n",
    "            print (\"Read document failed.\")\n",
    "        # print(scp_doc.data[\"affiliation\"])\n",
    "        return scp_doc.data\n",
    "    except:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thusly, the configuration file is set up and contains an APIkey. Further information: https://github.com/ElsevierDev/elsapy/blob/master/CONFIG.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "con_file = open(\"config.json\")\n",
    "config = json.load(con_file)\n",
    "con_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Moreover, the client is initialized with the API-Key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ElsClient(config['apikey'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For demonstation purposes, the following cells shows which data is returned by the Scopus API. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"affiliation\": [\n",
      "    {\n",
      "      \"affiliation-city\": \"New Delhi\",\n",
      "      \"affilname\": \"Jamia Hamdard\",\n",
      "      \"affiliation-country\": \"India\"\n",
      "    },\n",
      "    {\n",
      "      \"affiliation-city\": \"New Delhi\",\n",
      "      \"affilname\": \"Jamia Millia Islamia\",\n",
      "      \"affiliation-country\": \"India\"\n",
      "    },\n",
      "    {\n",
      "      \"affiliation-city\": \"New Delhi\",\n",
      "      \"affilname\": \"Indraprastha Apollo Hospitals\",\n",
      "      \"affiliation-country\": \"India\"\n",
      "    }\n",
      "  ],\n",
      "  \"coredata\": {\n",
      "    \"srctype\": \"j\",\n",
      "    \"eid\": \"2-s2.0-85083171050\",\n",
      "    \"pubmed-id\": \"32305024\",\n",
      "    \"prism:coverDate\": \"2020-07-01\",\n",
      "    \"prism:aggregationType\": \"Journal\",\n",
      "    \"prism:url\": \"https://api.elsevier.com/content/abstract/scopus_id/85083171050\",\n",
      "    \"dc:creator\": {\n",
      "      \"author\": [\n",
      "        {\n",
      "          \"ce:given-name\": \"Raju\",\n",
      "          \"preferred-name\": {\n",
      "            \"ce:given-name\": \"Raju\",\n",
      "            \"ce:initials\": \"R.\",\n",
      "            \"ce:surname\": \"Vaishya\",\n",
      "            \"ce:indexed-name\": \"Vaishya R.\"\n",
      "          },\n",
      "          \"@seq\": \"1\",\n",
      "          \"ce:initials\": \"R.\",\n",
      "          \"@_fa\": \"true\",\n",
      "          \"affiliation\": {\n",
      "            \"@id\": \"60019286\",\n",
      "            \"@href\": \"https://api.elsevier.com/content/affiliation/affiliation_id/60019286\"\n",
      "          },\n",
      "          \"ce:surname\": \"Vaishya\",\n",
      "          \"@auid\": \"6602902951\",\n",
      "          \"author-url\": \"https://api.elsevier.com/content/author/author_id/6602902951\",\n",
      "          \"ce:indexed-name\": \"Vaishya R.\"\n",
      "        }\n",
      "      ]\n",
      "    },\n",
      "    \"link\": [\n",
      "      {\n",
      "        \"@_fa\": \"true\",\n",
      "        \"@rel\": \"self\",\n",
      "        \"@href\": \"https://api.elsevier.com/content/abstract/scopus_id/85083171050\"\n",
      "      },\n",
      "      {\n",
      "        \"@_fa\": \"true\",\n",
      "        \"@rel\": \"scopus\",\n",
      "        \"@href\": \"https://www.scopus.com/inward/record.uri?partnerID=HzOxMe3b&scp=85083171050&origin=inward\"\n",
      "      },\n",
      "      {\n",
      "        \"@_fa\": \"true\",\n",
      "        \"@rel\": \"scopus-citedby\",\n",
      "        \"@href\": \"https://www.scopus.com/inward/citedby.uri?partnerID=HzOxMe3b&scp=85083171050&origin=inward\"\n",
      "      }\n",
      "    ],\n",
      "    \"source-id\": \"5700165201\",\n",
      "    \"pii\": \"S1871402120300771\",\n",
      "    \"citedby-count\": \"203\",\n",
      "    \"prism:volume\": \"14\",\n",
      "    \"subtype\": \"ar\",\n",
      "    \"dc:title\": \"Artificial Intelligence (AI) applications for COVID-19 pandemic\",\n",
      "    \"openaccess\": \"1\",\n",
      "    \"prism:issn\": \"18780334 18714021\",\n",
      "    \"prism:issueIdentifier\": \"4\",\n",
      "    \"subtypeDescription\": \"Article\",\n",
      "    \"prism:publicationName\": \"Diabetes and Metabolic Syndrome: Clinical Research and Reviews\",\n",
      "    \"prism:pageRange\": \"337-339\",\n",
      "    \"prism:endingPage\": \"339\",\n",
      "    \"openaccessFlag\": \"true\",\n",
      "    \"prism:doi\": \"10.1016/j.dsx.2020.04.012\",\n",
      "    \"prism:startingPage\": \"337\",\n",
      "    \"dc:identifier\": \"SCOPUS_ID:85083171050\",\n",
      "    \"dc:publisher\": \"Elsevier Ltd\"\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "return_example = AffiliationsFromScopusByDOI(client, '10.1016/j.dsx.2020.04.012')\n",
    "print(json.dumps(return_example, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the returned data, further analysis is conductable. Therefore, two notebooks are created to analyse data linked to: \n",
    "<ul>\n",
    "  <li>affiliation</li>\n",
    "  <li>coredata</li>\n",
    "</ul>    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thusly, the already fetched coredata and affiliation are read and combined to a DataFrame for further processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_current_extra_info = pd.DataFrame()\n",
    "try:\n",
    "    read_affiliation = pd.read_pickle('extra_info_affiliation_CS.pkl')\n",
    "    read_coredata = pd.read_pickle('extra_info_coredata_CS.pkl')\n",
    "    df_current_extra_info['affiliation'] = read_affiliation\n",
    "    df_current_extra_info['coredata'] = read_coredata\n",
    "    df_current_extra_info\n",
    "except:\n",
    "    print(\"The DataFrame is empty\")\n",
    "    #if the dataframe is not empty set the variable to show the dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The length of the DataFrame containing the current information is assigned to a variable to be used for further processing. \n",
    "Therefore, the length will be used within a while loop as a starting index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23190"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len_df_current_extra_info = len(df_current_extra_info)\n",
    "len_df_current_extra_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_current_extra_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_fetched_data_to_df(df_current_extra_info, dic):\n",
    "    \"\"\"\n",
    "    This function appends or inserts newly fetched data to the DataFrame containing scopus data.\n",
    "    \"\"\"\n",
    "    #df_current_extra_info -> holding the latest data, new data needs to be appended to it, \n",
    "    #df_newly_fetched_transposed -> holdy newly fetched data, needs to be inserted or fetched\n",
    "    \n",
    "    #Prior appending, the dictionary is transformed to a DataFrame\n",
    "    df_newly_fetched = pd.DataFrame(dic)\n",
    "    \n",
    "    #For readability, the DataFrame is transposed\n",
    "    df_newly_fetched_transposed = df_newly_fetched.T\n",
    "    \n",
    "    #Insert newly fetched rows which were previously not successful appended\n",
    "    for index, row in df_newly_fetched_transposed.iterrows():\n",
    "        #insert to current extra info DataFrame because the row is existent\n",
    "        if index in df_current_extra_info.index and row.affiliation is not None:\n",
    "            df_current_extra_info.loc[index] = row\n",
    "        #append to current extra info DataFrame because the row is new     \n",
    "        if index not in df_current_extra_info.index:\n",
    "            df_current_extra_info = df_current_extra_info.append(row, ignore_index=True)\n",
    "            \n",
    "    #returning DataFrame with inserted and replaced rows. \n",
    "    return df_current_extra_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both Dataframes columns are stored each to an object. The series objects are stored to each to a pkl-file which is not exceeding the size of 100MB allowing GitHub uploads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_df_columns(df):\n",
    "    ser_affiliation = df['affiliation']\n",
    "    ser_coredata = df['coredata']\n",
    "    ser_affiliation.to_pickle('extra_info_affiliation_CS.pkl')\n",
    "    ser_coredata.to_pickle('extra_info_coredata_CS.pkl')\n",
    "    return ser_affiliation, ser_coredata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Subsequently, the fetched scopus data is stored within a dictionary. Besides, the print function is used to show the state of the process by displaying the latest fetched information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Position fetched: 23190 -> 10.1016/j.jcv.2009.06.026\n",
      "Position fetched: 23191 -> 10.1016/j.jcv.2009.07.015\n",
      "Position fetched: 23192 -> 10.1016/j.jcv.2009.09.030\n",
      "Position fetched: 23193 -> 10.1016/j.jcv.2009.11.015\n",
      "Position fetched: 23194 -> 10.1016/j.jcv.2009.11.026\n",
      "Position fetched: 23195 -> 10.1016/j.jcv.2010.01.001\n",
      "Position fetched: 23196 -> 10.1016/j.jcv.2010.02.013\n",
      "Position fetched: 23197 -> 10.1016/j.jcv.2010.04.005\n",
      "Position fetched: 23198 -> 10.1016/j.jcv.2010.07.002\n",
      "Position fetched: 23199 -> 10.1016/j.jcv.2010.07.005\n",
      "Position fetched: 23200 -> 10.1016/j.jcv.2010.07.016\n",
      "Position fetched: 23201 -> 10.1016/j.jcv.2010.08.005\n",
      "Position fetched: 23202 -> 10.1016/j.jcv.2010.09.022\n",
      "Position fetched: 23203 -> 10.1016/j.jcv.2010.10.010\n",
      "Position fetched: 23204 -> 10.1016/j.jcv.2011.01.018\n",
      "Position fetched: 23205 -> 10.1016/j.jcv.2011.04.010\n",
      "Position fetched: 23206 -> 10.1016/j.jcv.2011.05.024\n",
      "Position fetched: 23207 -> 10.1016/j.jcv.2011.07.015\n",
      "Position fetched: 23208 -> 10.1016/j.jcv.2011.08.017\n",
      "Position fetched: 23209 -> 10.1016/j.jcv.2011.09.006\n",
      "Position fetched: 23210 -> 10.1016/j.jcv.2011.11.011\n",
      "Position fetched: 23211 -> 10.1016/j.jcv.2011.12.020\n",
      "Position fetched: 23212 -> 10.1016/j.jcv.2012.01.008\n",
      "Position fetched: 23213 -> 10.1016/j.jcv.2012.04.018\n",
      "Position fetched: 23214 -> 10.1016/j.jcv.2012.06.019\n",
      "Position fetched: 23215 -> 10.1016/j.jcv.2012.06.023\n",
      "Position fetched: 23216 -> 10.1016/j.jcv.2012.09.012\n",
      "Position fetched: 23217 -> 10.1016/j.jcv.2012.10.005\n",
      "Position fetched: 23218 -> 10.1016/j.jcv.2012.10.013\n",
      "Position fetched: 23219 -> 10.1016/j.jcv.2012.11.010\n",
      "Position fetched: 23220 -> 10.1016/j.jcv.2013.02.011\n",
      "Position fetched: 23221 -> 10.1016/j.jcv.2013.03.004\n",
      "Position fetched: 23222 -> 10.1016/j.jcv.2013.03.011\n",
      "Position fetched: 23223 -> 10.1016/j.jcv.2013.04.002\n",
      "Position fetched: 23224 -> 10.1016/j.jcv.2013.04.011\n",
      "Position fetched: 23225 -> 10.1016/j.jcv.2013.04.014\n",
      "Position fetched: 23226 -> 10.1016/j.jcv.2013.05.017\n",
      "Position fetched: 23227 -> 10.1016/j.jcv.2013.06.037\n",
      "Position fetched: 23228 -> 10.1016/j.jcv.2013.06.042\n",
      "Position fetched: 23229 -> 10.1016/j.jcv.2013.09.016\n",
      "Position fetched: 23230 -> 10.1016/j.jcv.2013.12.008\n",
      "Position fetched: 23231 -> 10.1016/j.jcv.2014.01.020\n",
      "Position fetched: 23232 -> 10.1016/j.jcv.2014.02.005\n",
      "Position fetched: 23233 -> 10.1016/j.jcv.2014.02.010\n",
      "Position fetched: 23234 -> 10.1016/j.jcv.2014.03.012\n",
      "Position fetched: 23235 -> 10.1016/j.jcv.2014.03.021\n",
      "Position fetched: 23236 -> 10.1016/j.jcv.2014.06.004\n",
      "Position fetched: 23237 -> 10.1016/j.jcv.2014.06.025\n",
      "Position fetched: 23238 -> 10.1016/j.jcv.2014.08.010\n",
      "Position fetched: 23239 -> 10.1016/j.jcv.2014.08.019\n",
      "Position fetched: 23240 -> 10.1016/j.jcv.2014.08.023\n",
      "Position fetched: 23241 -> 10.1016/j.jcv.2014.10.006\n",
      "Position fetched: 23242 -> 10.1016/j.jcv.2014.10.013\n",
      "Position fetched: 23243 -> 10.1016/j.jcv.2014.10.017\n",
      "Position fetched: 23244 -> 10.1016/j.jcv.2014.10.021\n",
      "Position fetched: 23245 -> 10.1016/j.jcv.2014.11.006\n",
      "Position fetched: 23246 -> 10.1016/j.jcv.2014.11.010\n",
      "Position fetched: 23247 -> 10.1016/j.jcv.2014.11.011\n",
      "Position fetched: 23248 -> 10.1016/j.jcv.2014.11.018\n",
      "Position fetched: 23249 -> 10.1016/j.jcv.2014.12.006\n",
      "Position fetched: 23250 -> 10.1016/j.jcv.2015.01.006\n",
      "Position fetched: 23251 -> 10.1016/j.jcv.2015.01.007\n",
      "Position fetched: 23252 -> 10.1016/j.jcv.2015.01.016\n",
      "Position fetched: 23253 -> 10.1016/j.jcv.2015.02.010\n",
      "Position fetched: 23254 -> 10.1016/j.jcv.2015.03.011\n",
      "Position fetched: 23255 -> 10.1016/j.jcv.2015.03.022\n",
      "Position fetched: 23256 -> 10.1016/j.jcv.2015.03.024\n",
      "Position fetched: 23257 -> 10.1016/j.jcv.2015.05.004\n",
      "Position fetched: 23258 -> 10.1016/j.jcv.2015.05.015\n",
      "Position fetched: 23259 -> 10.1016/j.jcv.2015.05.022\n",
      "Position fetched: 23260 -> 10.1016/j.jcv.2015.05.029\n",
      "Position fetched: 23261 -> 10.1016/j.jcv.2015.06.005\n",
      "Position fetched: 23262 -> 10.1016/j.jcv.2015.06.078\n",
      "Position fetched: 23263 -> 10.1016/j.jcv.2015.06.082\n",
      "Position fetched: 23264 -> 10.1016/j.jcv.2015.07.309\n",
      "Position fetched: 23265 -> 10.1016/j.jcv.2015.08.014\n",
      "Position fetched: 23266 -> 10.1016/j.jcv.2015.08.022\n",
      "Position fetched: 23267 -> 10.1016/j.jcv.2015.10.001\n",
      "Position fetched: 23268 -> 10.1016/j.jcv.2015.10.016\n",
      "Position fetched: 23269 -> 10.1016/j.jcv.2015.10.024\n",
      "Position fetched: 23270 -> 10.1016/j.jcv.2015.11.002\n",
      "Position fetched: 23271 -> 10.1016/j.jcv.2015.12.001\n",
      "Position fetched: 23272 -> 10.1016/j.jcv.2016.04.008\n",
      "Position fetched: 23273 -> 10.1016/j.jcv.2016.04.019\n",
      "Position fetched: 23274 -> 10.1016/j.jcv.2016.05.015\n",
      "Position fetched: 23275 -> 10.1016/j.jcv.2016.06.002\n",
      "Position fetched: 23276 -> 10.1016/j.jcv.2016.08.253\n",
      "Position fetched: 23277 -> 10.1016/j.jcv.2016.10.005\n",
      "Position fetched: 23278 -> 10.1016/j.jcv.2017.01.004\n",
      "Position fetched: 23279 -> 10.1016/j.jcv.2017.01.010\n",
      "Position fetched: 23280 -> 10.1016/j.jcv.2017.02.010\n",
      "Position fetched: 23281 -> 10.1016/j.jcv.2017.03.003\n",
      "Position fetched: 23282 -> 10.1016/j.jcv.2017.04.001\n",
      "Position fetched: 23283 -> 10.1016/j.jcv.2017.05.016\n",
      "Position fetched: 23284 -> 10.1016/j.jcv.2017.07.004\n",
      "Position fetched: 23285 -> 10.1016/j.jcv.2017.07.019\n",
      "Position fetched: 23286 -> 10.1016/j.jcv.2017.08.014\n",
      "Position fetched: 23287 -> 10.1016/j.jcv.2017.12.002\n",
      "Position fetched: 23288 -> 10.1016/j.jcv.2018.01.005\n",
      "Position fetched: 23289 -> 10.1016/j.jcv.2018.01.017\n",
      "Position fetched: 23290 -> 10.1016/j.jcv.2018.01.019\n",
      "Position fetched: 23291 -> 10.1016/j.jcv.2018.02.010\n",
      "Position fetched: 23292 -> 10.1016/j.jcv.2018.04.012\n",
      "Position fetched: 23293 -> 10.1016/j.jcv.2018.05.006\n",
      "Position fetched: 23294 -> 10.1016/j.jcv.2018.06.003\n",
      "Position fetched: 23295 -> 10.1016/j.jcv.2018.07.002\n",
      "Position fetched: 23296 -> 10.1016/j.jcv.2018.07.010\n",
      "Position fetched: 23297 -> 10.1016/j.jcv.2018.09.008\n",
      "Position fetched: 23298 -> 10.1016/j.jcv.2018.09.009\n",
      "Position fetched: 23299 -> 10.1016/j.jcv.2018.10.003\n",
      "Position fetched: 23300 -> 10.1016/j.jcv.2018.11.004\n",
      "Position fetched: 23301 -> 10.1016/j.jcv.2018.11.006\n",
      "Position fetched: 23302 -> 10.1016/j.jcv.2018.12.005\n",
      "Position fetched: 23303 -> 10.1016/j.jcv.2019.01.006\n",
      "Position fetched: 23304 -> 10.1016/j.jcv.2019.03.015\n",
      "Position fetched: 23305 -> 10.1016/j.jcv.2019.03.016\n",
      "Position fetched: 23306 -> 10.1016/j.jcv.2019.04.003\n",
      "Position fetched: 23307 -> 10.1016/j.jcv.2019.04.005\n",
      "Position fetched: 23308 -> 10.1016/j.jcv.2019.05.007\n",
      "Position fetched: 23309 -> 10.1016/j.jcv.2019.06.006\n",
      "Position fetched: 23310 -> 10.1016/j.jcv.2019.07.001\n",
      "Position fetched: 23311 -> 10.1016/j.jcv.2019.08.005\n",
      "Position fetched: 23312 -> 10.1016/j.jcv.2019.104200\n",
      "Position fetched: 23313 -> 10.1016/j.jcv.2019.104204\n",
      "Position fetched: 23314 -> 10.1016/j.jcv.2019.104206\n",
      "Position fetched: 23315 -> 10.1016/j.jcv.2019.104232\n",
      "Position fetched: 23316 -> 10.1016/j.jcv.2020.104261\n",
      "Position fetched: 23317 -> 10.1016/j.jcv.2020.104288\n",
      "Position fetched: 23318 -> 10.1016/j.jcv.2020.104305\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "i = len_df_current_extra_info\n",
    "dict_new_extra_info = dict()\n",
    "len_dois = len(doi_counted)\n",
    "while i < len_dois: ####################################################################### -> upto modified, normally len_dois\n",
    "    print(\"Position fetched: \" + str(i) + \" -> \" +  doi_counted.index[i])\n",
    "    dict_new_extra_info[i] = AffiliationsFromScopusByDOI(client, doi_counted.index[i])\n",
    "    i = i + 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following cell is useful when the process above is interrupted. Therefore, the dictionary containing fetched information can be narrowed down to useful entries. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_new_extra_info(len_df_current_extra_info, upto):\n",
    "    \"\"\"\n",
    "    This function is used to separate successfull API calls from API calls which were prevented due to an invalid API-Key.\n",
    "    As a result, this function returns a range of valid entries up to the given parameter. \n",
    "    \"\"\"\n",
    "    dict_new_extra_info_saver = dict()\n",
    "    i = len_df_current_extra_info\n",
    "    while i < upto:\n",
    "        #print(\"Position: \" + str(i) + \" -> \" +  doi_counted.index[i])\n",
    "        dict_new_extra_info_saver[i] = dict_new_extra_info[i]\n",
    "        i = i + 1 \n",
    "    return dict_new_extra_info_saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_fetched_data_to_df(df_current_extra_info, dic):\n",
    "    \"\"\"\n",
    "    This function appends or inserts newly fetched data to the DataFrame containing scopus data.\n",
    "    \"\"\"\n",
    "    #df_current_extra_info -> holding the latest data, new data needs to be appended to it, \n",
    "    #df_newly_fetched_transposed -> holdy newly fetched data, needs to be inserted or fetched\n",
    "    \n",
    "    #Prior appending, the dictionary is transformed to a DataFrame\n",
    "    df_newly_fetched = pd.DataFrame(dic)\n",
    "    \n",
    "    #For readability, the DataFrame is transposed\n",
    "    df_newly_fetched_transposed = df_newly_fetched.T\n",
    "    \n",
    "    #Insert newly fetched rows which were previously not successful appended\n",
    "    for index, row in df_newly_fetched_transposed.iterrows():\n",
    "        #insert to current extra info DataFrame because the row is existent\n",
    "        if index in df_current_extra_info.index and row.affiliation is not None:\n",
    "            df_current_extra_info.loc[index] = row\n",
    "        #append to current extra info DataFrame because the row is new     \n",
    "        if index not in df_current_extra_info.index:\n",
    "            df_current_extra_info = df_current_extra_info.append(row, ignore_index=True)\n",
    "            \n",
    "    #returning DataFrame with inserted and replaced rows. \n",
    "    return df_current_extra_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The existing and newly fetched information are combined into one DataFrame. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined_extra_info = append_fetched_data_to_df(df_current_extra_info, dict_new_extra_info)\n",
    "df_combined_extra_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to big for GitHub\n",
    "#df_combined_extra_info.to_csv('extra_info_CS5099.csv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both Dataframes columns are stored each to an object. The series objects are stored to each to a pkl-file which is not exceeding the size of 100MB allowing GitHub uploads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_df_columns(df):\n",
    "    ser_affiliation = df['affiliation']\n",
    "    ser_coredata = df['coredata']\n",
    "    ser_affiliation.to_pickle('extra_info_affiliation_CS.pkl')\n",
    "    ser_coredata.to_pickle('extra_info_coredata_CS.pkl')\n",
    "    return ser_affiliation, ser_coredata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_series = store_df_columns(df_combined_extra_info)\n",
    "stored_series[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stored_series[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verifying that the returned None values are due to non existent data and not to an invalid API-Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len_data = len(stored_series[0])\n",
    "# len_data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ser_doi = pd.Series(doi_counted.index[:len_data])\n",
    "# ser_doi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# df_current_extra_info_checker = df_combined_extra_info\n",
    "# df_current_extra_info_checker['doi'] = ser_doi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# len_df_current_extra_info_checker = len(df_current_extra_info_checker)\n",
    "# dict_new_extra_info_checker = dict()\n",
    "# i = 0 \n",
    "# while i < len_df_current_extra_info_checker: ###################################################### \n",
    "#     if df_current_extra_info_checker['affiliation'][i] == None:\n",
    "#         dict_new_extra_info_checker[i] = AffiliationsFromScopusByDOI(client, ser_doi[i])\n",
    "#         print(\"Position fetched again: \" + str(i) + \" -> \" +  ser_doi[i])\n",
    "#     i = i + 1    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_new_extra_info_checker\n",
    "# -> check if at least one value is not None -> otherwise the process is finished here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(dict_new_extra_info_checker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_combined_extra_info_fetched_again  = append_fetched_data_to_df(df_current_extra_info, dict_new_extra_info_checker)\n",
    "# df_combined_extra_info_fetched_again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# store_df_columns(df_combined_extra_info_fetched_again)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_combined_extra_info_fetched_again['check_doi'] = ser_doi\n",
    "# df_combined_extra_info_fetched_again.head(30)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
